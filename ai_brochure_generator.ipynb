{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6801ccf",
   "metadata": {},
   "source": [
    "# AI-Powered Brochure Generator (Website Scraper + Intelligent Navigation)\n",
    "\n",
    "This notebook generates a clean **Markdown brochure** for a company by:\n",
    "1) Scraping the homepage\n",
    "2) Selecting high-signal pages (About, Product, Solutions, Pricing, Customers, Careers)\n",
    "3) Optionally using an LLM to write a polished brochure\n",
    "\n",
    "Outputs go to `outputs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a309351",
   "metadata": {},
   "source": [
    "## What \"intelligent navigation\" means\n",
    "\n",
    "Company sites have messy menus. I combine:\n",
    "- **Heuristics**: prioritize common high-signal pages (about, product, solutions, pricing, customers, careers)\n",
    "- **Optional LLM ranking**: pick the best pages from candidates and return structured JSON\n",
    "\n",
    "You can run:\n",
    "- **Heuristics-only** (no API key)\n",
    "- **Heuristics + LLM** (better page selection and brochure quality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efee02",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the next cell once. It installs dependencies and creates a basic repo-friendly folder layout:\n",
    "\n",
    "- `src/` (optional export of the core code)\n",
    "- `outputs/` (generated brochures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running in a fresh environment, uncomment the pip installs.\n",
    "# %pip install -U requests beautifulsoup4 lxml tiktoken python-dotenv pydantic\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "Path(\"src\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Ready. Folders created: outputs/, src/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3a247",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set environment variables (recommended) or edit the defaults below.\n",
    "\n",
    "### For OpenAI (optional)\n",
    "- `OPENAI_API_KEY`\n",
    "- `OPENAI_MODEL` (example: `gpt-4.1-mini`)\n",
    "\n",
    "Tip: in a real repo, store these in a `.env` file and **do not commit it**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load a .env file if you use one locally (not required in hosted notebooks)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_MODEL   = os.getenv(\"OPENAI_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "print(\"OPENAI_API_KEY set:\", bool(OPENAI_API_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7aacc",
   "metadata": {},
   "source": [
    "## GitHub notes\n",
    "\n",
    "- Keep secrets out of Git: use environment variables or a local `.env` file (and add `.env` to `.gitignore`)\n",
    "- Commit the notebook and `outputs/` (optional) or keep `outputs/` ignored if you only want source in the repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbec627",
   "metadata": {},
   "source": [
    "## Core utilities: fetching, parsing, and cleaning\n",
    "\n",
    "I keep things polite and predictable:\n",
    "- reasonable user-agent\n",
    "- timeouts\n",
    "- max pages to crawl\n",
    "- ignore non-http links, mailto, tel, javascript, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54127d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import urllib.parse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional, Iterable\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"ai-brochure-generator/1.0 (+https://github.com/yourname/yourrepo)\"\n",
    "}\n",
    "\n",
    "def normalize_url(base_url: str, href: str) -> Optional[str]:\n",
    "    \"\"\"Return an absolute, clean URL or None if it should be ignored.\"\"\"\n",
    "    if not href:\n",
    "        return None\n",
    "    href = href.strip()\n",
    "\n",
    "    # Ignore anchors / non-web schemes\n",
    "    if href.startswith(\"#\"):\n",
    "        return None\n",
    "    bad_prefixes = (\"mailto:\", \"tel:\", \"javascript:\", \"data:\")\n",
    "    if href.lower().startswith(bad_prefixes):\n",
    "        return None\n",
    "\n",
    "    abs_url = urllib.parse.urljoin(base_url, href)\n",
    "    parsed = urllib.parse.urlparse(abs_url)\n",
    "\n",
    "    if parsed.scheme not in (\"http\", \"https\"):\n",
    "        return None\n",
    "\n",
    "    # Drop fragments\n",
    "    parsed = parsed._replace(fragment=\"\")\n",
    "    return parsed.geturl()\n",
    "\n",
    "def same_site(a: str, b: str) -> bool:\n",
    "    \"\"\"True if URLs share the same netloc (domain).\"\"\"\n",
    "    return urllib.parse.urlparse(a).netloc == urllib.parse.urlparse(b).netloc\n",
    "\n",
    "def fetch_html(url: str, timeout: int = 20) -> str:\n",
    "    resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urls = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        u = normalize_url(base_url, a.get(\"href\"))\n",
    "        if u:\n",
    "            urls.append(u)\n",
    "    # de-dupe while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Remove obvious non-content elements\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    # Clean whitespace\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a042b19",
   "metadata": {},
   "source": [
    "## Link scoring (heuristics)\n",
    "\n",
    "I give extra points to high-signal pages and penalize low-signal ones.\n",
    "\n",
    "This is intentionally simple and transparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_SIGNAL = [\n",
    "    (\"about\", 4.0),\n",
    "    (\"company\", 3.0),\n",
    "    (\"product\", 4.0),\n",
    "    (\"platform\", 3.0),\n",
    "    (\"solutions\", 4.0),\n",
    "    (\"pricing\", 5.0),\n",
    "    (\"customers\", 4.0),\n",
    "    (\"case-stud\", 4.0),\n",
    "    (\"security\", 2.0),\n",
    "    (\"trust\", 2.0),\n",
    "    (\"careers\", 3.0),\n",
    "    (\"jobs\", 3.0),\n",
    "    (\"team\", 2.0),\n",
    "    (\"contact\", 1.0),\n",
    "]\n",
    "\n",
    "LOW_SIGNAL = [\n",
    "    (\"blog\", -3.0),\n",
    "    (\"news\", -2.0),\n",
    "    (\"press\", -2.0),\n",
    "    (\"events\", -2.0),\n",
    "    (\"privacy\", -1.0),\n",
    "    (\"terms\", -1.0),\n",
    "    (\"cookie\", -1.0),\n",
    "    (\"login\", -5.0),\n",
    "    (\"signin\", -5.0),\n",
    "    (\"signup\", -4.0),\n",
    "    (\"status\", -2.0),\n",
    "    (\"docs\", -1.5),  # docs can be useful, but often too detailed for a brochure\n",
    "]\n",
    "\n",
    "def heuristic_score(url: str) -> float:\n",
    "    u = url.lower()\n",
    "    score = 0.0\n",
    "    for k, w in HIGH_SIGNAL:\n",
    "        if k in u:\n",
    "            score += w\n",
    "    for k, w in LOW_SIGNAL:\n",
    "        if k in u:\n",
    "            score += w\n",
    "    # Slight preference for shorter paths\n",
    "    path = urllib.parse.urlparse(url).path\n",
    "    score -= 0.02 * len(path)\n",
    "    return score\n",
    "\n",
    "def rank_links(links: List[str], top_k: int = 20) -> List[Tuple[str, float]]:\n",
    "    scored = [(u, heuristic_score(u)) for u in links]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b49bed",
   "metadata": {},
   "source": [
    "## Optional LLM step: pick best links and generate brochure\n",
    "\n",
    "Two LLM calls:\n",
    "1) **Pick links**: choose the best pages for a brochure from the candidate list\n",
    "2) **Write brochure**: synthesize scraped text into a brochure in Markdown\n",
    "\n",
    "If you do not have an API key, skip this and run heuristics-only mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_import_openai():\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        return OpenAI\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def llm_select_links(\n",
    "    company_name: str,\n",
    "    home_url: str,\n",
    "    candidate_links: List[str],\n",
    "    max_pages: int = 6\n",
    ") -> List[str]:\n",
    "    \"\"\"Use an LLM to pick the most relevant links. Requires OPENAI_API_KEY.\"\"\"\n",
    "    OpenAI = try_import_openai()\n",
    "    if not OpenAI or not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OpenAI client not available or OPENAI_API_KEY missing\")\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    system = (\n",
    "        \"You are a careful analyst. You select the most relevant company website pages \"\n",
    "        \"to build a marketing brochure for prospective clients, investors, and recruits.\"\n",
    "    )\n",
    "\n",
    "    prompt = {\n",
    "        \"company_name\": company_name,\n",
    "        \"home_url\": home_url,\n",
    "        \"instructions\": (\n",
    "            \"Pick the best pages for a brochure. Prefer About, Product/Platform, Solutions, \"\n",
    "            \"Pricing, Customers/Case Studies, Careers/Team, Security/Trust, Contact. \"\n",
    "            \"Avoid login pages, blog/news, policies. Return JSON only.\"\n",
    "        ),\n",
    "        \"max_pages\": max_pages,\n",
    "        \"candidate_links\": candidate_links,\n",
    "        \"output_schema\": {\"selected_links\": [\"https://...\"]}\n",
    "    }\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":json.dumps(prompt, indent=2)}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Robust-ish JSON extraction\n",
    "    m = re.search(r\"\\{.*\\}\", content, re.S)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Model did not return JSON: {content[:500]}\")\n",
    "    data = json.loads(m.group(0))\n",
    "    links = data.get(\"selected_links\", [])\n",
    "    # Basic validation\n",
    "    out = []\n",
    "    for u in links:\n",
    "        if isinstance(u, str) and u.startswith((\"http://\",\"https://\")):\n",
    "            out.append(u)\n",
    "    # de-dupe\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            cleaned.append(u)\n",
    "    return cleaned[:max_pages]\n",
    "\n",
    "def llm_write_brochure(company_name: str, source_pages: Dict[str, str]) -> str:\n",
    "    \"\"\"Use an LLM to write a brochure in Markdown. Requires OPENAI_API_KEY.\"\"\"\n",
    "    OpenAI = try_import_openai()\n",
    "    if not OpenAI or not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OpenAI client not available or OPENAI_API_KEY missing\")\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Keep the input bounded. In production you'd do chunking + summarization.\n",
    "    max_chars_per_page = 12000\n",
    "    compact = {u: t[:max_chars_per_page] for u, t in source_pages.items()}\n",
    "\n",
    "    system = (\n",
    "        \"You are a senior product marketer. You write crisp, factual brochures. \"\n",
    "        \"Do not invent facts. If something is unknown, omit it.\"\n",
    "    )\n",
    "\n",
    "    user = {\n",
    "        \"company_name\": company_name,\n",
    "        \"task\": (\n",
    "            \"Create a brochure in Markdown with these sections: \"\n",
    "            \"1) One-line summary, 2) What they do, 3) Key products/solutions, \"\n",
    "            \"4) Differentiators, 5) Proof (customers/case studies if available), \"\n",
    "            \"6) Pricing (only if found), 7) Security/compliance (only if found), \"\n",
    "            \"8) Careers snapshot (only if found), 9) CTA / next steps.\"\n",
    "        ),\n",
    "        \"source_pages\": compact\n",
    "    }\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":json.dumps(user)}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b9994",
   "metadata": {},
   "source": [
    "## Crawl pipeline\n",
    "\n",
    "This ties everything together:\n",
    "- fetch homepage\n",
    "- collect links\n",
    "- rank links\n",
    "- optionally ask LLM to pick final pages\n",
    "- crawl selected pages and extract text\n",
    "- optionally ask LLM to write brochure\n",
    "- save to `outputs/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b346f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrawlConfig:\n",
    "    max_candidate_links: int = 50\n",
    "    max_pages: int = 6\n",
    "    timeout: int = 20\n",
    "    sleep_s: float = 0.6  # be polite\n",
    "    use_llm_for_links: bool = False\n",
    "    use_llm_for_brochure: bool = False\n",
    "\n",
    "def crawl_for_brochure(company_name: str, home_url: str, cfg: CrawlConfig) -> Dict[str, str]:\n",
    "    home_html = fetch_html(home_url, timeout=cfg.timeout)\n",
    "    links = extract_links(home_url, home_html)\n",
    "    links = [u for u in links if same_site(home_url, u)]\n",
    "\n",
    "    ranked = rank_links(links, top_k=cfg.max_candidate_links)\n",
    "    candidate_links = [u for u, _ in ranked]\n",
    "\n",
    "    if cfg.use_llm_for_links:\n",
    "        selected = llm_select_links(company_name, home_url, candidate_links, max_pages=cfg.max_pages)\n",
    "    else:\n",
    "        selected = candidate_links[:cfg.max_pages]\n",
    "\n",
    "    pages: Dict[str, str] = {}\n",
    "    for u in selected:\n",
    "        try:\n",
    "            html = fetch_html(u, timeout=cfg.timeout)\n",
    "            pages[u] = html_to_text(html)\n",
    "        except Exception as e:\n",
    "            pages[u] = f\"[ERROR fetching {u}: {e}]\"\n",
    "        time.sleep(cfg.sleep_s + random.random()*0.2)\n",
    "\n",
    "    return pages\n",
    "\n",
    "def save_markdown(md_text: str, filename: str) -> str:\n",
    "    out_path = Path(\"outputs\") / filename\n",
    "    out_path.write_text(md_text, encoding=\"utf-8\")\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a6ed8",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Pick a company and run.\n",
    "\n",
    "Notes:\n",
    "- This notebook will only fetch pages if your runtime has internet access.\n",
    "- For a GitHub repo, consider adding unit tests and a CLI wrapper (easy upgrade path).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fbdde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (replace with a real company + homepage)\n",
    "company_name = \"Example Company\"\n",
    "home_url = \"https://example.com\"\n",
    "\n",
    "cfg = CrawlConfig(\n",
    "    max_candidate_links=40,\n",
    "    max_pages=6,\n",
    "    timeout=20,\n",
    "    sleep_s=0.6,\n",
    "    use_llm_for_links=bool(OPENAI_API_KEY),        # auto-enable if key exists\n",
    "    use_llm_for_brochure=bool(OPENAI_API_KEY),     # auto-enable if key exists\n",
    ")\n",
    "\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d524e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crawl\n",
    "# pages = crawl_for_brochure(company_name, home_url, cfg)\n",
    "# print(\"Crawled pages:\", len(pages))\n",
    "# list(pages.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate brochure (heuristics-only mode just dumps extracted text as a starting point)\n",
    "def simple_brochure_fallback(company_name: str, pages: Dict[str, str]) -> str:\n",
    "    lines = [f\"# {company_name} Brochure (Draft)\", \"\"]\n",
    "    lines.append(\"## Source pages\") \n",
    "    for u in pages.keys():\n",
    "        lines.append(f\"- {u}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Extracted content (raw)\")\n",
    "    for u, t in pages.items():\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"### {u}\")\n",
    "        lines.append(t[:6000])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# brochure_md = llm_write_brochure(company_name, pages) if cfg.use_llm_for_brochure else simple_brochure_fallback(company_name, pages)\n",
    "# out_file = save_markdown(brochure_md, f\"{company_name.lower().replace(' ','_')}_brochure.md\")\n",
    "# out_file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
